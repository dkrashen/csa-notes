
\documentclass[12pt]{report}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% package and document formatting stuff
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{amsmath,amsthm,amsfonts,amscd,amssymb}
\usepackage{makeidx,enumerate}

\usepackage{fullpage,color}
\usepackage[pdfstartview=FitH,
%             pdfauthor={\myauthor},
%             pdftitle={\mytitle},
            colorlinks,
            linkcolor=reference,
            citecolor=citation,
            urlcolor=e-mail,
            backref]{hyperref}
\usepackage[all]{xy}

\definecolor{todo}{rgb}{.80,.20,.20}
\definecolor{e-mail}{rgb}{0,.40,.80}
\definecolor{reference}{rgb}{.10,.40,.42}
\definecolor{mrnumber}{rgb}{.80,.40,0}
\definecolor{citation}{rgb}{0,.40,.80}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% theorem stuff
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\theoremstyle{plain}

\newtheorem{thm}{Theorem}[section]
\newtheorem{defn}[thm]{Definition}
\newtheorem{notn}[thm]{Notation}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{aside}[thm]{Aside}
\newtheorem{rem}[thm]{Remark}
\newtheorem{ex}[thm]{Example}
\newtheorem{facts}[thm]{Facts}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{prop}[thm]{Proposition}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% typography stuff
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\mb}[1]{\mathbf #1}
\newcommand{\mbb}[1]{\mathbb #1}
\newcommand{\mf}[1]{\mathfrak #1}
\newcommand{\mc}[1]{\mathcal #1}
\newcommand{\ms}[1]{\mathscr #1}
\newcommand{\mcu}[1]{\mathcu #1}
\newcommand{\oper}[1]{\operatorname{#1}}

\newcommand{\da}{\downarrow}
\newcommand{\ra}{\rightarrow}
\newcommand{\hra}{\hookrightarrow}
\newcommand{\dra}{\dashrightarrow}
\newcommand{\la}{\leftarrow}
\newcommand{\lra}{\longrightarrow}

\newcommand{\ov}{\overline}
\newcommand{\til}{\widetilde}
\newcommand{\wh}{\widehat}

\newcommand{\ZZ}{\mathbb{Z}}

\newcommand{\ann}{\oper{ann}}
\newcommand{\coker}{\oper{coker}}
\newcommand{\End}{\oper{End}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% other stuff
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\makeindex
\newcommand{\X}[1]{#1\index{#1}}
\newcommand{\Xb}[1]{\textbf{#1}\index{#1}}

\newcommand{\todo}[1]{\textcolor{todo}{#1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% end preamble 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% title stuff
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\author{Daniel Krashen}
\title{Central Simple Algebras}

\maketitle
\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% document stuff
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Algebraic Preliminaries}

\section{Notation and conventions}

\subsection{Rings and conventions}

Rings are not necessarily commutative. They are always assumed to be
associative and unital. Ring homomorphisms are required to be unital. The
elements $1$ and $0$ need not be distinct. The ring $R$ itself is a
(non-proper) ideal.

\subsection{Modules and bimodules}

\begin{defn}
Let $R$ be a ring. A left $R$-module is a set $M$ together with a binary
operation
\begin{align*}
R \times M &\to M \\
(r, m) &\to rm
\end{align*}
such that
\begin{enumerate}[1. ]
\item $1 m = m$, 
\item $(r_1 r_2) m = r_1 (r_2 m)$, 
\item $(r_1 + r_2)m = r_1m + r_2 m$
\item $r(m_1 + m_2) = rm_1 + rm_2$
\end{enumerate}
\end{defn}

\begin{defn}
Let $R$ be a ring. A right $R$-module is a set $M$ together with a binary
operation
\begin{align*}
M \times R &\to M \\
(m, r) &\to mr
\end{align*}
such that
\begin{enumerate}[1. ]
\item $m 1 = m$, 
\item $m (r_1 r_2) = (m r_1) r2$, 
\item $m (r_1 + r_2)m = m r_1 + m r_2$
\item $(m_1 + m_2)r = m_1 r + m_2 r$
\end{enumerate}
\end{defn}

\begin{notn}
We will occasionally write $M_R$ (respectively $_R M$) to denote the fact
that $M$ is a right (respectively left) $R$-module.
\end{notn}

\begin{rem}
Recall that for a ring $R$, we may define its opposite $R^{op}$ as the ring
with the same underlying set and addition, but with the new multiplication
rule $\cdot$ defined by $r \cdot s = sr$. In this way, we see
that if $M$ is a left $R$ module, then we may define the structure of a
right $R^{op}$ module on $M$ via $m \cdot r = rm$. This gives an
equivalence of categories between left (right) $R$-modules and right (left)
$R^{op}$ modules.
\end{rem}

\begin{defn}
Let $R, S$ be rings. An $R-S$ \X{bimodule} is a set $M$ endowed with a left
$R$-module structure and a right $S$-module structure such that for all $r
\in R, s \in S, m \in M$, we have
\[ r(m s) = (r m) s.\]
\end{defn}

\begin{rem}
We note that just as every Abelian group naturally has the structure of a
$\ZZ$-module, every left (resp. right) $R$-module has the structure of a
$R-\ZZ$ (resp. $\ZZ - R$) bimodule.
\end{rem}

\begin{notn}
We will write $_R M_S$ to denote the fact that $M$ is an $R-S$ bimodule.
\end{notn}


\section{Some Structure Theory}

\subsection{Simple and Semisimple Modules}

\begin{defn}
Let $R$ be a ring. We say that a left $R$-module $P$ is \X{simple} if it is nonzero
and if the only submodules of $P$ are $0$ and $P$.
\end{defn}

\begin{defn}
Let $R$ be a ring, $P$ a left $R$-module. For a subset $X \subset P$, we
define $\ann_R(X)$, the \X{annihilator} of $P$ in $R$, to be the set
\[\ann_R(X) = \{r \in R | rX = 0\}.\]
\end{defn}

Note that $\ann_R(X)$ is itself always a left ideal of $R$. Further, in the
case $X = P$, we find that $\ann_R(P)$ is a two-sided ideal of $R$.

\begin{defn} 
A left $R$-module $M$ is called \X{faithful} if $\ann_R(M) = 0$.
\end{defn}

\begin{defn} 
An ideal $I < R$ is called left \X{primitive} if $I$ is of the form $I =
\ann_R(P)$ for some simple left $R$ module $P$.
\end{defn}

\begin{prop} \label{simple modules maximal ideals}
Suppose that $P$ is a nonzero right $R$-module. The following are equivalent:
\begin{enumerate}[1. ]
\setlength{\itemsep}{0cm}
\item \label{simple 1} $P$ is simple,
\item \label{simple 2} for every $m \in P\setminus\{0\}$, $mR = P$,
\item \label{simple 3} $P \cong R/I$ for $I$ a maximal right ideal of $R$,
\end{enumerate}
\end{prop}
\begin{proof}
\mbox{}

\noindent
(\ref{simple 1} $\implies$ \ref{simple 2})
Suppose $P$ is a simple right $R$ module, and let $m \in
P\setminus\{0\}$. Then $mR$ is a nonzero submodule of $P$ and hence we must
have $mR = P$. 

\noindent
(\ref{simple 2} $\implies$ \ref{simple 3})
Choose some $m \in P\setminus\{0\}$.
By hypothesis, we have a surjective right $R$-module map
\begin{align*}
R &\to P \\
r &\mapsto mr,
\end{align*}
and it follows that $P \cong R/\ann_R(m)$. By the correspondence theorem,
since $P$ is simple, it follows that $\ann_R(m)$ must be a maximal right
ideal of $R$.

\noindent
(\ref{simple 3} $\implies$ \ref{simple 1})
Follows immediately from the definition of a maximal ideal.
\end{proof}


\begin{defn} 
Let $R$ be a ring. We say that a left $R$-module $P$ is \X{semisimple} if it is
a direct sum of simple modules.
\end{defn}

\begin{prop} \label{semisimple sub quot}
Let $A$ be an algebra over a field $F$, $M$ a semisimple left $A$-module,
finite dimensional as an $F$ vector space, and $P < M$ a submodule. Then
$P$ and $M/P$ are also semisimple. Further, we may find a submodule $L
\subset M$ such that $L \oplus P = M$.
\end{prop}
We note that the finite dimensionality assumption is not necessary if one
appeals to Zorn's Lemma, but we will keep it for simplicity of exposition.
\begin{proof}
Since $M$ is semisimple, we may write $M = \oplus M_i$ where $M_i$ are
simple. By finite dimensionality, the number of summands is finite.  Let $Q
< M/P$ be maximal dimensional so that $Q$ is semisimple. Arguing by
contradiction, assume that $Q \neq M/P$. It follows that we may find some
$M_i$ with the image of $M_i$ in $M/P$ (i.e.  $(M_i + P)/P$ not contained
in $Q$. Set $Q_i = (M_i + P)/P$. Then as before, we have $Q \oplus Q_i <
M/P$ a semisimple module of larger dimension.

For the remaining parts, choose $k$ minimal such that there exists a
decomposition $M = \oplus_{i = 1}^n M_i$ with each $M_i$ simple, such that the
projection $\pi: P \to M \to N = \oplus_{i = 1}^k M_i$ is injective. We claim
that $\pi$ is an isomorphism. It suffices to show that it is surjective.
Regarding $M_i$ as a submodule of $N$, we note that $\pi P \cap M_i \neq 0$
for each $i$, since otherwise, the projection onto $\oplus_{\stackrel{j =
1}{j \neq i}}^k M_j$ would still be injective, contradicting the minimality
of $k$. It therefore follows that, since $M_i$ is simple, each $M_i$ is a
submodule of $\pi P$, for $i = 1, \ldots, k$, and hence $N \subset \pi P$.
But since the reverse inclusion holds by definition, we have $\pi P = N$
and hence $\pi$ is bijective. This gives an isomorphism $N \cong P$,
proving the semismiplicity of $P$.

Finally, consider $L = \oplus_{i = k+1}^n M_i$. We claim that $L \cap P =
0$. To see this, suppose that $x \in L \cap P$. Then by
definition of $\pi$, it follows that $\pi x = 0$. However, $\pi$ is
injective, and so $x = 0$ as claimed. Next, to finish, we show that $L
+ P = M$. Choosing $m \in M$, we may write $m = \ell + n$ for $\ell \in L$
and $n \in N$. Since $\pi$ is an isomorphism, we cna write $n = \pi p$, and
consequently, we have $p = n + x$ for $x \in L$. We therefore have
\[m = \ell + n = \ell + p - x = (\ell - x) + p \in L + P\]
as desired.
\end{proof}

\subsection{Semiprimitive Algebras}

\begin{defn} 
Let $R$ be a ring. We define $J_r(R)$ (respectively $J_\ell(R)$), the right
(left) \X{Jacobson radical} of $R$, to be the intersection of all the maximal
right (left) ideals of $R$. 
\end{defn}
In fact, we will show eventually that the right and left Jacobson radicals
coincide.

Note that since the annihilator of any element in a simple module is a
maximal ideal and every maximal ideal is the annihilator of some element in
some simple module, it follows that the right Jacobson radical can also be
characterized as the set of elements of $R$ which annihilate every simple
right module.

\begin{lem}
Let $R$ be a ring. Then $J_r(R)$ is a two sided ideal of $R$.
\end{lem}
\begin{proof}
If $M$ is a simple right module for $R$, then $\ann_R(M)$ is a two sided
ideal. Since $J_r(R)$ is the intersection of all such ideals, it is itself
an ideal.
\end{proof}

\begin{lem}
Suppose that $A$ is a finite dimensional $F$ algebra. Then $A_A$ is a
semisimple right $A$ module if and only if $J_r(A) = 0$.
\end{lem}
\begin{proof}
Suppose that $A_A$ is a semisimple module. Then we may write $A = \oplus
P_i$ for some right ideal $P_i$'s which are simple as right $A$-modules.
Consequently, if we define $P_i' = \oplus_{j \neq i} P_i$, then $P_i'$ is a
right $A$-module with $A/P_i' \cong P_i$ simple, and so $P_i'$ is a maximal
ideal. But the intersection of the $P_i'$ is $0$ which implies $J_r(A) =
0$.

Conversely, if we assume that $J_r(A) = 0$. Since $A$ is finite
dimensional, we may find a finite collection of maximal ideals $M_i$ with
$\cap M_i = 0$. But this implies that that map $A \to \oplus A/M_i$ is
injective, and hence $A$ is isomorphic, as a right $A$-module, to a
submodule of a semisimple module. By Proposition~\ref{semisimple sub quot},
it follows that $A_A$ is semisimple as desired. 
\end{proof}

\subsection{An ambidextrous characterization of the Jacobson radical}

Recall that $r \in R$ is called left invertible if there is some $s \in R$ so
that $sr = 1$, and right invertible if there is some $t \in R$ so that $rt
= 1$. The elements $s$ and $t$ in these cases are called, respectively,
left and right inverses for $R$. In general it is possible to be right, but
not left invertible (or the reverse), and it is not true in general that a
one-sided inverse must be unique.

\begin{ex}
Let $V$ be the vector space of real valued infinite sequences $(a_0, a_1,
\ldots)$, and let $R$ be the ring of linear transformations on $R$. The
linear transformations
\begin{align*}
\sigma, \tau : V &\to V \\
\sigma(a_0, a_1, a_2, \ldots) &= (0, a_0, a_1, \ldots) \\
\tau(a_0, a_1, a_2, \ldots) &= (a_1, a_2, a_3, \ldots) \\
\gamma(a_0, a_1, a_2, \ldots) &= (a_0, a_0, a_1, \ldots),
\end{align*}
satisfy $\tau \sigma = \tau \gamma = id$, so that $\sigma$ and $\gamma$
are both right inverses for $\tau$. However since as a function, $\tau$ is
not injective, it follows that it cannot have a left inverse.
\end{ex}

\begin{aside}
This situation described above is an ``infinite dimensional phemomenon.''
In particular, if $A$ is a finite dimensional algebra over a field $F$,
then if $a \in A$ has a right (left) inverse, it must also have a left
(right) inverse. 
\end{aside}
\begin{proof}
To see this, we note that if $a$ has a right inverse, then
it must be, as a linear transformation from $A$ to itself, surjective. By
finite dimensionality, it must therefore also be injective, and hence
invertible as a linear transformation. This means that its determinant must
be nonzero. If we consider its characteristic polynomial (as a linear
transformation),
\[\chi_a(t) = t^n + c_{n-1} t^{n-1} + \cdots + c_0\]
then we have $c_0 = \pm det(a) \neq 0$, and since $\chi_a(a) = 0$,
by the Cayley-Hamilton Theorem, we have
\[(-a_0^{-1})(a^{n-1} + c_{n-1} a^{n-2} + \cdots c_1) a = 1\]
and hence $a$ has a left inverse as well. In fact, it quickly follows both
from this explicit description, as well as the next result, that its right
and left inverse are the same.
\end{proof}

In general, if an element of a ring has both a right and a left inverse,
these must coincide and be unique:
\begin{lem}
Let $R$ be a ring, $r \in R$ and suppose that $s, t \in R$ with $sr = 1 =
rt$. Then $s = t$.
\end{lem}
\begin{proof}
We have $s = s1 = srt = 1t = t$.
\end{proof}
If $r \in R$ has both a left and a right inverse, we simply say that it is
invertible, and can speak of its uniquely defined inverse.

\begin{defn} 
Let $R$ be a ring, and $r \in R$. We say that $r$ is left \X{quasiregular}
if $1 - r$ has a left inverse, right quasiregular if $1 - r$ has a right
inverse, and simply quasiregular it is both left and right quasiregular. 
\end{defn}

\begin{lem}
Suppose that $I$ is a right ideal all of whose element are right
quasiregular. Then all of its elements are quasiregular.
\end{lem}
\begin{proof}
Let $x \in I$. We have by hypothesis that $(1 - x)s = 1$. Writing $y = 1 -
s$ we may write this as $(1 - x)(1 - y) = 1$ and so $xy - x - y = 0$,
yielding $y = -x(1 - y) \in I$. Consequently, $y$ is right quasiregular,
and it follows that $(1 - y)$ is right invertible. But since $(1 - x)$ is a
left inverse for $(1 - y)$, it is invertible with $(1 - x)$. But this means
that $(1 - x)$ is also invertible with inverse $(1 - y)$. This means that
$x$ is quasiregular as claimed.
\end{proof}

\begin{lem}
Let $R$ be a ring. Then every element $x \in J_r(R) \cup J_\ell(R)$ is
quasiregular.
\end{lem}
\begin{proof}
By the previous result, and by symmetry, it suffices to show that every
element of $J_r(R)$ is right quasiregular. Let $x \in J_r(R)$. Since $x$ is
contained in every maximal right ideal, it follows that $1 - x$ is
contained in no maximal ieals. But this implies that the right ideal
generated by $1 - x$ must be the whole right $R$, which tells us in turn
that it is right invertible, and hence $x$ is right quasiregular as
claimed.
\end{proof}

\begin{lem}
Suppose that $I$ is an ideal of $R$ such that every element of $I$ is
quasiregular. Then $I \subset J_r(R) \cap J_\ell(R)$.
\end{lem}
\begin{proof}
By symmetry, it suffices to show that $I \subset J_r(R)$.
Suppose that $K$ is a maximal right ideal of $R$, and consider $K + I$. We
will show that $I \subset K$ by contradiction. Since $J_r(R)$ is the
intersection of all maximal ideals, it will follow that $I \subset J_r(R)$.
If $I \not\subset K$, we would have, by maximality of $K$, that 
$K + I = R$. But then we can write $1 = k + x$, with $k \in K$ and $x \in I$, so
that $k = 1 - x$. But since $x$ is quasiregular, $k$ is invertible and
hence $K = R$ would not be maximal. Therefore $I \subset K$ as desired.
\end{proof}

\begin{cor}
Let $R$ be a ring. Then $J_r(R) = J_\ell(R)$ is the ideal of $R$ which is
maximal with respect to the property that each of its elements are
quasiregular.
\end{cor}

It therefore makes sense to define the \X{Jacobson radical} of $R$, to be $J(R)
= J_r(R) = J_\ell(R)$.

\begin{defn}
We say that a ring $R$ is \Xb{semiprimitive} if $J(R) = 0$.
\end{defn}

\subsection{Endomorphisms: Schur and Wedderburn-Artin}

The main observation of this section is that if $R$ is a ring, then
regarding $R$ as a right module over itself, we have a natural
identification $R \cong End_R(R_R)$. This means that by studying the
structure of Endomorphism rings of modules, we can get to the structure of
arbitrary rings.

Let us first consider the structure of endomorphism rings of simple
modules:
\begin{thm}[Schur's Lemma] 
Let $P$ be a simple right $R$ module, and let $D = End_R(P_R)$. Then $D$ is
a division ring.
\end{thm}
\begin{proof}
Suppose that $f \in D \setminus \{0\}$. We must show that $f$ is invertible.
But note that $f$, considered as a homomorphism from $P$ to itself, has a
kernel and image which are both submodules of $P$. Since $P$ has no
submodules other than $0$ and $P$, and since $f$ is not the $0$ map, it
follows that the kernel must be $0$ and the image must be $P$. But this
implies that $f$ is both injective and surjective, and hence $f$ is
invertible as a map of sets. Writing $g$ for $f^{-1}$, one may check that
$g$ is also a right $R$-module homomorphism, and hence $g \in D$ is an
inverse for $f$ as desired.
\end{proof}

To examine semisimple modules, it will be useful to consider matrix
notation. Let $M = \oplus_{j = 1}^m M_j$ and $N = \oplus_{i = 1}^n N_i$ be
right $R$-modules, each written as a finite direct sum. If $f : M \to N$ is
a homomorphism, $f$ is determined by its values on each of the submodules
$M_j$. Moreover, $f_j = f|_{M_j}$ can be written as a tuple of maps
$(f_{1,j}, f_{2, j}, \ldots, f_{m, j})$ where each $f_{i, j}$ is a
homomorphism from $M_j$ to $N_i$. We may represent this in matrix notation
as follows:
\[
f = \left[
\begin{matrix}
f_{1, 1} & f_{1, 2} & \cdots & f_{1, n} \\
f_{2, 1} & f_{2, 2} & \cdots & f_{2, n} \\
\vdots & \vdots & & \vdots \\
f_{m, 1} & f_{m, 2} & \cdots & f_{m, n}
\end{matrix}
\right]
\]
and one may check that composition of functions $fg$ precisely corresponds
to matrix multiplication and composition of endomorphisms within each entry
of the product. Consequently, we have:
\begin{lem}
Let $R$ be a ring, $M = \oplus_{j = 1}^m M_j$. Then $End_R(M)$ is
isomorphic to the ring of matrices of the form
\[
\left[
\begin{matrix}
Hom_R(M_1, M_1) & Hom_R(M_1, M_2) & \cdots & Hom_R(M_1, M_n) \\
Hom_R(M_2, M_1) & Hom_R(M_2, M_2) & \cdots & Hom_R(M_2, M_n) \\
\vdots & \vdots & & \vdots \\
Hom_R(M_m, M_1) & Hom_R(M_m, M_2) & \cdots & Hom_R(M_m, M_n)
\end{matrix}
\right]
\]
with the natural ring structure inhereted by matrix addition and
multiplication.
\end{lem}

\begin{thm}[Wedderburn-Artin] 
Let $A$ be a finite dimensional algebra over a field $F$, and suppose that
$J(A) = 0$. Then we may write $A = \oplus (P_i)^{d_j}$ as a direct sum of
minimal right ideals, and $A \cong \oplus_{i = 1}^n M_{d_i}(D_i)$, where
each $D_i = End_A(P_i)$ is a division algebra. 
\end{thm}
\begin{proof}
Since $J(A) = 0$, $A_A$ is a semisimple right $A$-module, and we can
write $A_A = \oplus_{i = 1}^n (P_i)^{d_i}$, where the $P_i$'s are distinct,
and mutually nonisomorphic simple right $R$-modules (and hence minimal
right ideals). Since the $P_i$'s are nonisomorphic and simple, it follows
that $Hom_{P_i, P_j} = 0$ if $i \neq j$ and is isomorphic to a division
algebra $D_i$ if $i = j$. It therefore follows that $End_A(A_A)$ consists
of block diagonal matrices with the algebras of the form $M_{d_i}(D_i)$
along the diagonal. The result follows.
\end{proof}

\begin{cor}[Wedderburn Structure Theorem] \label{simple wedderburn}
Let $A$ be a simple finite dimensional algebra over a field $F$. Then $A_A
= P^d$ for some minimal right ideal $P <_r A$ and some positive integer $d$.
Further, $A
\cong M_d(D)$ where $D = End_A(P)$ is a division algebra.
\end{cor}
\begin{proof}
Since $A$ is simple, we have $J(A) = 0$. By the Wedderburn-Artin Theorem,
it follows that $A \cong \oplus_i M_{d_i}(D_i)$, where each $D_i$ has the
form $End_A(P_i^{d_i})$. But since each of these factors would be an ideal
of $A$, and $A$ is simple, it follows that there is only one index $i$, and
so $A = P^d$, with $A \cong M_d(D)$ as claimed.
\end{proof}

\begin{cor} \label{unique simple module} 
Suppose that $A$ is a simple, finite dimensional algebra over a field $F$.
Then all simple right $A$ modules are isomorphic. In particular, all the
minimal right ideals of $A$ are isomorphic as right $A$-modules.
\end{cor}
\begin{proof}
Suppose that $P, Q$ are minimal right ideals which are nonisomorphic right
$A$-modules.  Since $A$ is simple, $J(A) = 0$ and $A_A$ is semisimple.
Write $A_A = \oplus P_i$. Since we have nontrivial homomorphisms $P, Q \to
A_A$, it follows that we must have nontrivial homomorphisms $P \to P_i$,
$Q \to P_j$ some $i, j$. But since all these are simple modules, we
therefore have $P \cong P_i$ and $Q \cong P_j$. By the Wedderburn-Artin
Theorem, it follows that we have at least two distinct factors in the
representation $A \cong \oplus_i M_{d_i}(D_i)$, contradicting the
simplicity of $A$.
\end{proof}

\section{Tensors and commutators}

\subsection{Tensor products of algebras}

\subsubsection{Definition and universal property} 
Let $F$ be a field, and $A, B$ $F$-algebras. Then the vector space tensor
product $A \otimes_F B$ can be given the structure of an $F$-algebra via,
defining for simple tensors:
\[(a \otimes b)(a' \otimes b') = aa' \otimes bb'.\]

Under this definition, we find that $A, B$ are naturally isomorphic to
subalgebras of $A \otimes_F B$, as $A \otimes 1$ and $B \otimes 1$
respectively, and furthermore, these images of the algebras commute inside
$A \otimes_F B$. In fact, this characterizes the tensor product of
algebras:

\begin{prop}
Suppose that $A, B$ are $F$-algebras. Then for any $F$-algebra $C$, the set
of homomorphisms $A \otimes B \to C$ are in bijection with pairs of
homomorphisms $A \to C$, $B \to C$ such that the images of $A$ and $B$ in
$C$ commute (via restriction to the subalgebras defined above).
\end{prop}
\begin{proof}
Clearly, if we are given a homomorphism $\phi: A \otimes B \to C$, we
obtain by restriction to the subalgebras $A \otimes 1$ and $1 \otimes B$,
corresponding homomoprhisms from $A$ and $B$. Since $A \otimes B$ is
generated by these subalgebras, it follows that the homomorphism $\phi$ is
determined by these restrictions. Since $a \otimes 1$ and $1 \otimes b$
commute in $A \otimes B$, it follows that the images of $A$ and $B$ commute
in $C$.

It remains simply to show that every pair of maps $\alpha : A \to C$,
$\beta : B \to C$ with $\alpha A$ commuting with $\beta B$ is induced by a
map $A \otimes B \to C$. To see this, consider the map
\[\phi: A \times B \to C\]
defined by $\phi (a, b) = ab$. This is a bilinear map and therefore induces
a map $\til \phi: A \otimes B \to C$ such that $\til \phi(a \otimes b) =
(\alpha a) (\beta b)$. To see that this is a homomorphisms, we check
\begin{align*}
\phi\left((\sum a_i \otimes b_i)(\sum a_j' \otimes b_j')\right) &= 
\phi \left(\sum_{i,j} a_i a_j' \otimes b_i b_j'\right) \\
&= \sum_{i,j} \phi(a_i
a_j' \otimes b_i b_j') \\
&= \sum_{i,j} \alpha(a_i a_j') \beta(b_i b_j') \\
&= \sum_{i,j} \alpha(a_i)\alpha(a_j') \beta(b_i)\beta(b_j') \\
&= \sum_{i,j} \alpha(a_i)\beta(b_i)\alpha(a_j') \beta(b_j') \\
&= \left(\sum \alpha(a_i)\beta(b_i)\right) \left(\sum
\alpha(a_j')\beta(b_j')\right) \\
&= \phi\left(\sum a_i \otimes b_i\right) \phi\left(\sum a_j' \otimes
b_j'\right)
\end{align*}
\end{proof}

\subsubsection{Description via structure constants}

\todo{
Suppose that $A$ is a finite dimensional $F$-algebra. if $\{e_i\}$ is a basis
for $A$, ....
}

\subsection{Tensors and bimodules}

Let $F$ be a field, $A$ and $B$ $F$-algebras. If $M$ is an $A-B$
bimodule then we have algebra homomorphisms $A \to \End_F(M)$ and
$B^{op} \to \End_F(M)$. The statement that these give a bimodule structure
is exactly the statement the the images of these maps commute in the
endomorphism ring. 

\begin{prop}
Let $A, B$ be $F$-algebras. Then we have natural equivalences between the
following categories
\begin{enumerate}[1. ]
\item $A-B$ bimodules,
\item $F$-algebra homomorphisms $A \otimes B^{op} \to \End_F(M)$,
\item $A \otimes B^{op}$-modules.
\end{enumerate}
\end{prop}

\subsection{Commutators and endomorphisms}

Recall that if $R$ is a ring, $L \subset R$ any subset, the \X{centralizer}
of $L$ in $R$, $C_R(L)$ is defined to be the set of $r \in R$ which commute
with every element of $L$.

Let $F$ be a field, $A$ an $F$-algebra, and $M$ a right $A$-module. The
right $A$-module structure of $M$ gives rise to a homomorphism of
$F$-algebras
\[f : A^{op} \to \End_F(M).\]
We can then characterize the endomorphisms of $M$ as a right $A$-module as
those linear transformations $b : M \to M$ such that
\[b(ma) = (b(m)) a, \]
or in other words, $\End_A(M) = C_{\End_F(M)}(f(A))$.

\begin{notn} \label{commutators associators}
If $A$ is an $F$-algebra, $M$ a right $A$-module, we regard elements of
$\End_A(M)$ as acting on the left of $M$, using the standard convention of
functions (in $\End_F(M)$) acting on the left. On the other hand, if $N$ is
a left $A$-module, we will instead choose to regard $A$-module
endomorphisms of $N$ as acting on the right, and therefore will consider
$N$ as a right $\End_A(N)^{op}$-module, following therefore the convention
of functions composing left to right instead of right to left.

That is to say, if $f, g \in \End_A(N)$, we will write $nf$ for $f(n)$, and
$nfg$ to represent $g(f(n)) = gf(n)$.
\end{notn}

\subsection{A double commutator theorem}

To warm up, let's examine what certain commutators look like in matrix
algebras.

\begin{lem} \label{commutator lemma}
Suppose that $R \subset S$ are rings. We may consider the inclusions 
\[R \subset S \subset M_n(S)\]
where the latter is induced by mapping the element $s$ to the diagonal
matrix
\(s I_n\). Then 
\begin{enumerate}[1. ]
\item \label{commutator 1} $C_{M_n(S)}(R) = M_n(C_S(R))$,
\item \label{commutator 2} $C_{M_n(S)}(M_n(R)) = C_S(R) I_n$.
\end{enumerate}
\end{lem}
\begin{proof}
It is easy to check that for a matrix $a = \sum a_{i,j} e_{i,j}$ and a
matrix $b = \sum \beta e_{i,i} = \beta I_n$, that $a$ and $b$ commute
exactly when $\beta$ commutes with each $a_{i,j}$. In particular, it
follows that $C_S(R) I_n \subset C_{M_n(S)}(M_n(R))$ and $M_n(C_S(R))
\subset C_{M_n(S)}(R)$.

To check that $C_{M_n(S)(R)} \subset M_n(C_S(R))$, choose $a = \sum_{i,j}
a_{i,j} e_{i,j}\in
C_{M_n(S)}(R)$. For $r \in R$, we have
\[0 = ar - ra = \sum (a_{i,j} r - r a_{i,j}) e_{i,j} \]
which tells us that $a_{i,j} \in C_S(R)$, as desired.

Finally, if we let $a = \sum_{i,j} a_{i,j} e_{i,j} \in C_{M_n(S)}(M_n(R))$,
then $a$ must commute with each matrix unit $e_{k,\ell}$. 
We therefore have 
\[0 = e_{k,\ell} a - a e_{k, \ell} = \sum_{i,j} a_{i,j} e_{k, \ell} e_{i,j} -
\sum_{i,j} a_{i,j} e_{i,j} e_{k,\ell} = \sum_j a_{\ell, j} e_{k, j} -
\sum_i a_{i, k} e_{i,\ell}.\]
Consequently, we must have that the
coefficient of a general $e_{p, q}$ is $0$. For $p \neq k$, this says that
$a_{p, k} = 0$ and for $q \neq \ell$, this says that $a_{\ell, q} = 0$.
It follows that such an $a$ must be diagonal, of the form $a
= \sum a_{i,i} e_{i,i}$. In this case, the commutator now looks like:
\[e_{k,\ell} a - a e_{k, \ell} = \sum_{i} a_{i,i} e_{k, \ell} e_{i,i} -
\sum_{i} a_{i,i} e_{i,i} e_{k,\ell} = a_{\ell, \ell} e_{k, \ell} - a_{k, k}
e_{k, \ell} = (a_{\ell, \ell} - a_{k,k}).\]
It then follows that for these to commute, $a$ must be of the form $s I_n$.
for some $s \in S$. Since $a$ must also commute with $R = R I_n$, we
therefore find $s \in C_S(R) I_n$ as claimed.
\end{proof}


\begin{thm}[Double Centralizer, Version 1] \label{dc1} 
Let $B$ be an $F$-algebra, and $M$ a faithful semisimple right $B$-module,
finite dimensional as an $F$-vector space. Let $E = \End_F(M)$, regard
$B^{op}$ as a subalgebra of $E$ via its right multiplication action. Then
we have $B^{op} = C_E(C_E(B^{op}))$.
\end{thm}
\begin{proof}
Let $\phi \in C_E(C_E(B^{op}))$, and choose $m_1, \ldots, m_n$ a basis for $M$ over
$F$. As in the convention of Notation~\ref{commutators associators}, $E$
and $C_E(B^{op})$
act on the left on $M$, and $C_E(C_E(B^{op}))$ act on the right. 

Set $N = \oplus^n M$, and let $w = (m_1, \ldots, m_n) \in N$. Since $N$ is
semisimple as a right $B$-module, we can write $N = wB \oplus N'$ as right
$B$-modules, and let $\pi : N \to mB \to N$ be the projection. The map $\phi \in
E$ acts naturally (diagonally) on $N$ as $\phi I_n \in M_n(E) = \End_F(N)$,
and since the $m_i$ are a basis, it follows that the action of $\phi$ on
$M$ is determined by its action on $w \in N$. It therefore suffices to show
that we can find $b \in B$ such that $w \phi = w b$.

To see this, we note that since $\pi$ is a right $B$-module homomorphism,
we have by Lemma~\ref{commutator lemma}(\ref{commutator 1}),
\[\pi \in C_{M_n(E)}(B^{op}) = M_n(C_E(B^{op})).\]
Further, we see that the action of $\phi$ on $N$ as $\phi I_n$ satisfies
\[\phi \in C_E(C_E(B^{op})) I_n = C_{M_n(E)}(M_n(C_E(B^{op}))),\]
by Lemma~\ref{commutator lemma}(\ref{commutator 2}). Consequently, the actions of $\pi$ and
$\phi$ on $N$ commute, and we have, since $w = \pi w$,
\[ w \phi = (\pi w) \phi = \pi(w \phi) \in wB.\]
This means we may write $w\phi = wb$ some $b \in B$, as desired.
\end{proof}

\chapter{The Structure of Central Simple Algebras}

\section{Characterizing central simple algebras}

\begin{defn}
Let $F$ be a field, and $A$ an $F$-algebra. We say that $A$ is $F$-central
if $Z(A) = F$ and $\dim_F(A)$ is finite.
\end{defn}

\begin{defn}
Let $F$ be a field and $A$ an $F$-algebra. We say that $A$ is a \X{central
simple algebra} over $F$ (a $csa/F$ for short), if
\begin{enumerate}[1. ]
\item $A$ is simple as a ring (no $2$-sided ideals) and
\item $A$ is $F$-central. 
\end{enumerate}
\end{defn}

A particularly important case of this is if $D$ is a finite dimensional
division algebra over $F$ with $Z(D) = F$. In this case, $D$ is called a
\X{central division algebra} over $F$ (a $cda/F$ for short).

\begin{prop}\label{central simple wedderburn artin}
Let $F$ be a field. The following are equivalent:
\begin{enumerate}[1. ]
\item \label{csa is a 1}
$A$ is a $csa/F$, 
\item \label{csa is a 2}
$A \cong M_n(D)$ for some $D$, a $cda/F$.
\end{enumerate}
Further, in part (\ref{csa is a 2}), $D$ is uniquely defined up to
isomorphism by $A$.
\end{prop}
\begin{proof}
Assuming that $A$ is a $csa/F$, it follows from the Wedderburn structure
Theorem (Corollary~\ref{simple wedderburn}), that we have $A \cong M_n(D)$
for some division algebra $D$ over $F$, where $D$ may be identified as
$\End_A(P)$ for a simple right module $P$, and $P$ is uniquely determined
up to isomorphism by Corollary~\ref{unique simple module}. Consequently, $D$ is uniquely determined up to
isomorphism by $A$. To see that $D$ is a $cda/F$, we note that by
Lemma~\ref{commutator lemma}(\ref{commutator 2}),
\[F = Z(A) = Z(M_n(D)) = C_{M_n(D)}(M_n(D)) = Z(D) I_n = Z(D). \]

On the other hand, assuming that $D$ is a $cda/F$, if $A = M_n(D)$ it
follows from computation with matrix units $e_{i,j}$ that $A$ is a simple
algebra. Again by Lemma~\ref{commutator lemma}(\ref{commutator 2}), we have
$Z(A) = Z(M_n(D)) = Z(D)$. Since $Z(D) = F$, this shows that $A$ is
central.
\end{proof}

Given any $F$-algebra $A$, we may regard $A$ has having the structure of an
$A-A$ bimodule. This induces an $F$-algebra homomorphism
\begin{align*}
A \otimes A^{op} &\to \End_F(A) \\
a \otimes b &\mapsto \big( x \mapsto axb \big)
\end{align*}
often referred to as the \X{sandwich map}.

\begin{thm} \label{sandwich csa}
Let $F$ be a field and $A$ a finite dimensional $F$-algebra. Then $A$ is a
$csa/F$ if and only if the sandwich map $A \otimes A^{op} \to \End_F(A)$ is
an isomorphism.
\end{thm}
\begin{proof}
On the one hand, suppose that the sandwich map is an isomorphism. We can
see that $A$ is simple since if it did have a proper ideal $I \triangleleft A$,
then $I \otimes A^{op}$ would be a proper ideal (one may verify properness
by considering its dimension as an $F$-vector space). However, tihs would
contradict the fact that  $\End_F(A) = M_{\dim A}(F)$ is a simple algebra.
To see that $A$ is central, we note that if $a \in Z(A)$, then 
\[a \otimes 1 \in Z(A \otimes A^{op}) = Z(\End_F(A)) = Z(M_{\dim A}(F)) =
F.\]
If $a$ is not in the $F$-span of $1 \in A$, then it would follow that $a
\otimes 1$ and $1 \otimes 1$ would be linearly independent and hence $a
\otimes 1$ would not be in the $F$-span of $1 \otimes 1 = 1_{A \otimes
A^{op}}$, contradicting the fact that $a \otimes 1 \in Z(A \otimes A^{op})
= F$. It therefore follows that $a \in F \cdot 1$, and so $Z(A) = F$ as
claimed.

Next suppose that $A$ is a $csa/F$. To see that the sandwich map is an
isomorphism, we note that since both sides are vector spaces over $F$ of
dimension $\left(\dim_F(A)\right)^2$, it suffices to check that the map is
surjective. Let $B \subset \End_F(A)$ be the image of $A \otimes A^{op}$.
Since $A$ is a simple algebra, it is a simple left $A \otimes A^{op}$
module, and hence a simple left $B$-module. 

Consider the centralizer $C_{\End_F(A)}(B)$. If $f: A \to A \in \End_F(A)$
is an element of this centralizer, then setting $x = f(1) \in A$, we find
that for $a \in A$, we have $f(a) = f((a \otimes 1) 1) = a \otimes 1 f(1) =
a \cdot x \cdot 1 = ax$, and so $f$ is determined by its value on $1$. On
the other hand, since $f$ also must commute with the right action of $A$
(i.e. the left module action of $A^{op}$ from the bimodule structure), we
have
\[ ax = f(a) = f((1 \otimes a) 1) = (1 \otimes a) f(1) = xa \]
for every $x \in A$, which tells us that $a \in Z(A) = F$. Consequently, we
find $C_{\End_F(A)}(B) = F$.

Since $A$ is a simple algebra, it is a simple left $A \otimes A^{op}$ module,
and it is a simple left $B$ module, and therefore also a semisimple
$B$-module. By the Double Centralizer Theorem, Version 1
(Theorem~\ref{dc1}), we therefore have have
\[B = C_{\End(A)}(C_{\End(A)}(B)) = C_{\End(A)}(F) = \End(A), \]
and so the sandwich map is surjective.
\end{proof}

\begin{thm}
Suppose that $A/F$ is an algebra. Then the following are equivalent:
\begin{enumerate}[1. ]
\item \label{is csa} $A$ is a $csa/F$,
\item \label{has sandwich} the sandwich map $A \otimes A^{op} \to
\End_F(A)$ is an isomorphism,
\item \label{has inverse} $A \otimes_F B \cong M_m(F)$ for some $F$-algebra
$B$ and some positive integer $m$,
\item \label{split at closure} $A \otimes_F \ov{F} \cong M_n(\ov F)$ for
some positive integer $n$,
\item \label{split somewhere} $A \otimes_F L \cong M_n(\ov F)$ for some
field extension $L/F$ and some positive integer $n$.
\end{enumerate}
\end{thm}
\begin{proof}
The equivalence of (\ref{is csa}) and (\ref{has sandwich}) follows from
Theorem~\ref{sandwich csa}, and the implication (\ref{has sandwich})
$\implies$ (\ref{has inverse}) is immediate.

To check (\ref{has inverse}) $\implies$ (\ref{split at closure}), we note
that the isomorphism $A \otimes B \cong M_n(F)$ yields an isomoprhism
$(A \otimes_F {\ov F}) \otimes_{\ov F} (B \otimes_F {\ov F})$, and so it
follows that $A \otimes_F \ov F$ is a central simple $\ov F$-algebra. By
the Wedderburn Structure Theorem (Corollary~\ref{simple wedderburn}), we
have $A \otimes \ov F \cong M_n(D)$ for some finite dimensional division
algebra $D$ over $\ov F$. But note that for every $d \in D$, $\ov F(d) \subset
D$ is a commutative finite dimensional domain, and hence a field. Since
this means that $\ov F(d)/\ov F$ is a finite field extension, and $\ov F$
is algebraically closed, it follows that $D = \ov F$ and so $A \otimes_F
\ov F \cong M_n(\ov F)$ as claimed.

Clearly (\ref{split at closure}) $\implies$ (\ref{split somewhere}), and so
to complete the proof, we need only show that (\ref{split somewhere})
$\implies$ (\ref{is csa}). So, suppose that $A \otimes_F L \cong M_n(L)$
for some field extension $L/F$. If $I \triangleleft A$ is a two-sided
ideal, then $I \otimes L$ is a two-sided ideal of $A \otimes L  = M_n(L)$ of
dimension $(\dim I)(\dim L)$. In particular, $I$ is proper if and only if
$I \otimes L$ is proper. Since $A \otimes L \cong M_n(L)$ is simple, it
follows that $A$ has no proper ideals and is therefore also simple.
Similarly, we note that if $a \in Z(A)$, then $a \otimes 1 \in Z(A \otimes
L) = 1 \otimes L$. By Proposition~\ref{tensor basis}, if $a \not\in F$,
then $a \otimes 1$ would be independent from the $F$-subspace $1 \otimes L$
in $A \otimes L$, and it would follow that $a \not\in Z(A)$ contradicting
our assumption. Therefore, we must have $a \in F$ and so $Z(A) = F$ as
claimed.

\end{proof}

\section{The degree and the index}


\section{The Brauer Group}

\todo{
brauer equivalence, identification with isomorphism classes of division
algebras. period. state period index problem.
}

\section{Noether-Skolem and Double Centralizers}

\section{Maximal Subfields}

\todo{open questions on the existence of nonmaximal subfields}

\section{Quaternions, Cyclics and Crossed Products}

\section{The Second Galois Cohomology Group}

\todo{period divides index}

\todo{primary decomposition in the Brauer group}

\todo{decomposibility - open problems and examples}

\section{Cyclicity of Division Algebras}

\subsection{Degree $3$ algebras}

\iffalse

\chapter{Involutions and Algebraic Groups}

\section{Hermitian forms and Witt groups}

\section{Orthogonal, Symplectic and Unitary groups}

\section{Adjoint involutions and similarity}

\section{Characterizing algebras with involution}

\todo{The Pfaffian and half-maximal subfields}

\section{Construction of Classicial groups}

\chapter{Ramification}

\chapter{Some Nice Fields}

\chapter{Polynomial Identities}

\chapter{The Generic Division Algebra}

\chapter{Geometric Methods}

\chapter{Obstructions in Geometry}

\fi

\appendix

\chapter{Tensors}


\section{Existence of Tensor products}

\begin{defn}
Let $R, S, T$ be rings, $_R M_S$, $_S N_T$, $_R P _T$ bimodules. We say
that a map
\[\phi : M \times N \to P\]
is $R-S-T$ linear if
\begin{enumerate}[1. ]
\item for all $n \in N$, the map
\begin{align*}
M &\to P \\
m &\mapsto \phi(m, n)
\end{align*}
is a left $R$-module map.
\item for all $m \in M$, the map
\begin{align*}
N &\to P \\
n &\mapsto \phi(m, n)
\end{align*}
is a right $T$-module map.
\item for all $n \in N, m \in M, s \in S$, we have $\phi(ns, m) = \phi(n,
sm)$.
\end{enumerate}
\end{defn}

\begin{defn}
Given bimodules $ _R M_S$, $_S N_T$, we say that a bimodule $ _R P_T$
together with an $R-S-T$ linear map $\phi: M \times N \to P$ is a tensor product
for $M$ and $N$ if for every other bimodule $_R Q_T$ and $R-S-T$ linear map
$\psi : M \times N \to Q$, there is a unique $R-T$ bimodule map $\alpha : P
\to Q$ such that we have a commutative diagram:
\[\xymatrix{
M \times N \ar[rd]_{\psi} \ar[rr]^\psi & & Q \\
 & P \ar[ru]_{\alpha}
}\]
\end{defn}

In fact, tensor products always exist and are unique up to unique
isomorphism. The uniqueness follows from the standard arguments of
universal objects, and the existence is a consequence of the following
explicit construction.

\begin{defn}
Let $\Lambda$ be a set. We define the free Abelian group
generated by $\Lambda$, denoted $\left<\Lambda\right>$ to be the set
of formal finite linear combinations
\[\sum_{i = 1}^n a_i \lambda_i , \lambda_i \in
\Lambda, a_i \in \ZZ \]
subject to the relation $a \lambda + b \lambda = (a + b) \lambda$.
\end{defn}

\begin{defn}
Given bimodules $_R M _S$, $_S N _T$, we define $M \otimes_S N$ to be the
quotient of $\left< M \times N \right>$ by the submodule generated by
the following types of expressions of the form $(ms, n) - (m, sn)$.
We write $m \otimes n$ to denote the equivalence class of $(m,
n)$ in $M \otimes N$. This has a $R-T$ bimodule structure induced by $r(m
\otimes n) = rm \otimes n$ and $(m \otimes n) t = m \otimes nt$.
\end{defn}

The map $M \times N \to M \otimes_S N$ sending $(m, n)$ to $m \otimes n$
gives $M \otimes N$ the structure of a tensor product of $M$ and $N$. We
refer to this as \textit{the} tensor product of $M$ and $N$.

\section{Scalar extension}

A particularly useful instance of the tensor product is when one has an
extension of rings $R \subset S$ (or more generally, a homomorphism of
rings $\phi : R \to S$), and a left $R$-module $M$. In this case, the
tensor product $S \otimes_R M$ naturally inherets the structure of a left
$S$ module (we are tensoring an $S-S$ bimodule with a $S-\ZZ$-bimodule).

We refer to $S \otimes_R M$ as the scalar extension (or base change) of $M$
to $S$.

\section{Tensor products of vector spaces}

The tensor product of vector spaces is particularly easy to describe. Note
first that if $V$ is an $F$-vector space, then since $F$ is commutative, we
may either regard $V$ as a right or as a left $F$-module. Doing both at
once is also an option since $F$ is commutative, and in this way $V$ can be
regarded as an $F-F$ bimodule. It follows then that the tensor product of
vector spaces also inherets a natural vector space structure.

To simplify our language (and comply with convention), we will refer to a
$F-F-F$ linear function as simply a bilinear function.

\begin{prop} \label{tensor basis}
Suppose that $V$ and $W$ are vector spaces over a field $F$ with bases
$\{v_i\}$ and $\{w_j\}$ respectively. Then $V \otimes_F W$ is a vector
space with basis $\{v_i \otimes w_j\}$.
\end{prop}
\begin{proof}
It is clear that these elements span: by the definition of the tensor
product, a typical element of $V \otimes_F W$ is of the form $\sum a_k
\otimes b_k$ for some $a_k \in V, b_k \in W$. In particular, to see that
our elements span, it suffices to show that any vector of the form $a
\otimes b$ lies in the span. By definition, we may write
\[ a = \sum a_i v_i, \ b = \sum b_j w_j \]
and so
\[ a \otimes b = (\sum a_i v_i) \otimes (\sum b_j w_j) = \sum_{i,j} a_i b_j
v_i \otimes w_j \]
is in the span, as claimed.

To check independence, consider the function $f_{k,\ell} : V \times W \to F$
defined by
\[ f_{k, \ell} (\sum a_i v_i, \sum b_j w_j) = a_k b_\ell.\]
It is easy to check that this is bilinear, and hence factors uniquely
through the tensor product. Write $\til f_{k, \ell} : V \otimes W \to F$ as
the induced linear transformation. We then have 
$\til f_{k, \ell} (v_i \otimes w_j) = \delta_{(k, \ell), (i, j)}$ 
and in particlar, 
$\til f_{k, \ell}(\sum c_{i,j} v_i \otimes w_j) = c_{k, \ell}$.

It follows that, if 
\[\alpha = \sum c_{i, j} v_i \otimes w_j = 0\]
then $\til f_{i, j}(\alpha) = c_{i, j} = 0$ for all $i, j$, showing that
these are indeed independent.
\end{proof}

\section{Base extension of maps}

\begin{defn}
Given a ring extension $R \subset S$ and a homomorphism of left $R$-modules
$f : M \to N$, we define 
\[S \otimes f : S \otimes_R M \to S \otimes_R N\]
to be the map given by $S \otimes f (s \otimes m) = s \otimes f(m)$, and
then extending by linearity to general elements of the tensor product.
\end{defn}

\begin{lem}
Suppose that $L/F$ is a field extension, and $V$ is a vector space over
$F$ with basis $\{v_i\}$. Then $\{1 \otimes v_i\}$ is a basis for $L
\otimes V$.
\end{lem}
\begin{proof}
It is easy to see that the elements $1 \otimes v_i$ span $L \otimes V$ over
$L$. To see that they are independent over $L$, consider an expression 
\[\alpha = \sum x_i \otimes v_i, \] 
with $x_i \in L$ for each $i$.

For any $j$, we may consider the function 
\[\phi_j: L \times V \to L\]
via $\phi_j(x, \sum a_i v_i) = xa_j$. Note that this is a bilinear map of
$F$-vector spaces, and hence defines an $F$-linear transformation $\til
phi_j: L \times V \to L$. 

We compute that $\phi_i(\alpha) = x_i$, and therefore if $\alpha = 0$, each
$\phi_i(\alpha) = x_i = 0$ as claimed.
\end{proof}

\begin{lem}
Suppose that $L/F$ is a field extension and $f : F^n \to F^m$ is a linear
transformation represented by a matrix $(a_{i,j})$. Then $L \otimes f$ is
represented by the matrix $(a_{i,j})$.
\end{lem}

\begin{lem}
Suppose that $L/F$ is a field extension, and $f: V \to W$ is a linear
transormation of $F$-vector spaces. Then
\[ \ker L \otimes f = L \otimes_F \ker f, \ \ \coker L \otimes f = L
\otimes_F \coker f \]
\end{lem}
\begin{proof}
This follows from the fact that bases for both of these can be computed
through Gaussian elimination using the matrix in some basis. Since the
matrix doesen't change under extension of scalars, the kernel and cokernel
have the corresponding basis over $F$ and $L$.
\end{proof}

%\bibliographystyle{alpha}
%\bibliography{citations}
\printindex

\end{document}
